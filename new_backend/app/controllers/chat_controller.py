from config.config import settings
from app.models.chat_model import ChatMessage
from fastapi.responses import StreamingResponse
from typing import List
import json
import httpx


class ChatController:
    def __init__(self, model: str = settings.DEFAULT_MODEL) -> None:
        self.__headers = {"Authorization": f"Bearer {settings.LLM_SERVICE_API_KEY}"}
        self.__payload = {
            "model": model, 
            "messages": [], 
            "stream": True
        }
      
    @property  
    def model(self) -> str | None:
        return self.__payload.get("model")
    
    @property
    def messages(self) -> str | None:
        return self.__payload.get("messages")
    
    @model.setter
    def model(self, model_name: str):
        self.__payload["model"] = model_name
        
    @messages.setter
    def messages(self, messages: List[ChatMessage]):
        self.__payload["messages"] = messages
        
    def append_message(self, message: ChatMessage):
        self.__payload["messages"].append(message)
        
    async def generate_chat(self):
        content_received = False
        had_error = False
        error_message = ""
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                async with client.stream("POST", settings.LLM_URL, headers=self.__headers, json=self.__payload) as response:
                    async for line in response.aiter_lines():
                        line = line.strip()
                        if line.startswith('data: '):
                            data_content = line[6:]
                            if data_content == '[DONE]':
                                if not content_received:
                                    had_error = True
                                    error_message = "AI generated no content - empty response"
                                    yield f"Error: {error_message}\n"
                                break
                            
                            try:
                                data_obj = json.loads(data_content)
                                if (data_obj.get("choices") and 
                                    data_obj["choices"][0].get("delta")):
                                    
                                    content = data_obj["choices"][0]["delta"].get("content")
                                    if content and content.strip():
                                        content_received = True
                                        yield content
                            except json.JSONDecodeError:
                                continue
                            
                    if not content_received and not had_error:
                        had_error = True
                        error_message = "Stream ended without generating content"
                        yield f"Error: {error_message}\n"
                    
        except Exception as e:
            had_error = True
            error_message = f"Stream error: {str(e)}"
            yield f"Error: {error_message}\n"
        
        if not content_received and not had_error:
            yield "Error: No content generated by AI model\n"
            
    async def stream_chat(self) -> StreamingResponse | ValueError:
        try:
            return StreamingResponse(self.generate_chat(), media_type="text/plain")
        except Exception as e:
            return ValueError(f"Error, there has been an error fetching from llm provider: {e}")
