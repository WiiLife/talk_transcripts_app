from fastapi import APIRouter, Request, HTTPException
from fastapi.responses import StreamingResponse
from dotenv import load_dotenv
import json
import os 
import httpx


dotenv_path = os.path.join(os.path.dirname(__file__), '..', '.env')
load_dotenv(dotenv_path)

LLM_URL = os.environ.get("LLM_URL", "https://openrouter.ai/api/v1/chat/completions")
LLM_SERVICE_API_KEY = os.environ.get("LLM_SERVICE_API_KEY", None)

route = APIRouter(prefix="/api/v1", tags=["llm_router"])

@route.post("/chat/completions")
async def get_chat_completions(request: Request):
    
    data: dict = await request.json()
        
    headers = {"Authorization": f"Bearer {LLM_SERVICE_API_KEY}"}
    payload = {
        "model": data.get("model"), 
        "messages": data.get("messages"), 
        "stream": True
    }
            
    async def generate():
        content_received = False
        had_error = False
        error_message = ""
        
        try:
            async with httpx.AsyncClient(timeout=60.0) as client:
                async with client.stream("POST", LLM_URL, headers=headers, json=payload) as response:
                    async for line in response.aiter_lines():
                        line = line.strip()
                        if line.startswith('data: '):
                            data_content = line[6:]
                            if data_content == '[DONE]':
                                if not content_received:
                                    had_error = True
                                    error_message = "AI generated no content - empty response"
                                    yield f"Error: {error_message}\n"
                                break
                            
                            try:
                                data_obj = json.loads(data_content)
                                if (data_obj.get("choices") and 
                                    data_obj["choices"][0].get("delta")):
                                    
                                    content = data_obj["choices"][0]["delta"].get("content")
                                    if content and content.strip():
                                        content_received = True
                                        yield content
                            except json.JSONDecodeError:
                                continue
                            
                    if not content_received and not had_error:
                        had_error = True
                        error_message = "Stream ended without generating content"
                        yield f"Error: {error_message}\n"
                    
        except Exception as e:
            had_error = True
            error_message = f"Stream error: {str(e)}"
            yield f"Error: {error_message}\n"
        
        if not content_received and not had_error:
            yield "Error: No content generated by AI model\n"
                        
    try:
        return StreamingResponse(generate(), media_type="text/plain")
    except Exception as e:
        return HTTPException(status_code=500, detail={f"Error, there has been an error fetching from llm provider: {e}"})

@route.get("/chat/completions-test")
async def test_chat_completions():
    headers = {
        "Authorization": f"Bearer {LLM_SERVICE_API_KEY}",
        "Content-Type": "application/json"
        }
    payload = {
        "model": "mistralai/mistral-7b-instruct:free",
        "messages": [{"role": "user", "content": "ping"}],
        "stream": False
    }
    
    return LLM_SERVICE_API_KEY
    
    async with httpx.AsyncClient() as client:
        response = await client.post(LLM_URL, headers=headers, json=payload)
        print(f"Test response status: {response.status_code}")
        print(f"Test response body: {response.text}")
        
        return response.json()
